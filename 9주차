경사하강법(Gradient Descent)은 기계 학습 및 최적화 문제에서 널리 사용되는 알고리즘이다.
이 알고리즘은 주어진 함수의 최소값(또는 최대값)을 찾기 위해 사용되는데, 경사하강법의 기본 개념은 함수의 기울기를 이용하여 함수 값이 감소하는 방향으로 이동하는 것이다.
이렇게하면 함수의 최솟값에 점점 가까워진다.

경사하강법은 다음과 같은 단계로 진행된다

1. 함수의 매개변수(또는 가중치)를 임의의 값으로 초기화한다.
2. 현재 매개변수에서의 손실 함수의 기울기를 계산한다.
3. 매개변수를 기울기의 반대 방향으로 이동시킨다. 이동하는 크기는 학습률(learning rate)이라는 하이퍼파라미터로 조절이 가능하다.
4. 기울기가 0에 가까워질 때까지 또는 지정된 횟수만큼 반복한다.

수식으로 표현하면 𝜃 = 𝜃 − 𝜂 ⋅ ∇𝜃 𝐽(𝜃)

θ는 매개변수다.
η는 학습률이다.
∇ θ​ J(θ)는 매개변수 θ에 대한 손실 함수 J(θ)의 기울기다.

학습률의 역할
η는 경사하강법의 중요한 하이퍼파라미터다.
학습률이 너무 크면 최솟값을 지나쳐서 발산할 수 있고, 학습률이 너무 작으면 수렴 속도가 매우 느려질 수 있다.
적절한 학습률을 선택하는 것이 중요하다.

장점: 비교적 단순하고 구현이 쉬우며, 대규모 데이터셋에서 효율적으로 작동한다.
단점: 학습률 선택이 중요하고, 부적절한 학습률은 수렴 문제를 일으킬 수 있다. 또한, 복잡한 손실 함수에서 지역 최솟값에 빠질 수 있다.

결론
경사하강법은 최적화 문제를 해결하는 기본적이면서도 강력한 방법이다.
다양한 변형들이 존재하며, 각 변형은 특정 상황에서 더 나은 성능을 발휘한다.
 기계 학습 및 딥러닝 모델의 성능을 극대화하는 데 필수적이다.


경사하강법을 이용하여 매개변수를 갱신하는 Optimizer 클래스를 설계하고, 이를 상속받는 SGD 클래스를 구현한다.
Optimizer 클래스는 기본적인 기능을 포함하고, 실제 매개변수 갱신 로직은 자식 클래스에서 구현한다.

###########################################################
Optimizer 클래스
class Optimizer:
    def __init__(self):
        self.hooks = []

    def add_hook(self, hook):
        self.hooks.append(hook)

    def update(self, params):
        for hook in self.hooks:
            hook(params)
        for param in params:
            self.update_one(param)

    def update_one(self, param):
        raise NotImplementedError()
###########################################################
SGD 클래스
class SGD(Optimizer):
    def __init__(self, lr=0.01):
        super().__init__()
        self.lr = lr

    def update_one(self, param):
        param.data -= self.lr * param.grad.data  # 경사하강법으로 매개변수 갱신
###########################################################
param.data -= self.lr * param.grad.data 문법은 파이썬에서 자기 자신을 갱신하는 연산이다. 이는 param.data = param.data - self.lr * param.grad.data와 동일하하다. 
즉, 매개변수 param.data에 학습률 lr과 기울기 param.grad.data의 곱을 빼서 갱신한다.

########################################################### 모델 및 옵티마이즈 생
import math
import numpy as np
import matplotlib.pyplot as plt
import dezero
from dezero import optimizers
import dezero.functions as F
from dezero.models import MLP

max_epoch = 300
batch_size = 30
hidden_size = 100  # 은닉층 노드 수
lr = 1.0  # 학습률

x, t = dezero.datasets.get_spiral(train=True)
model = MLP((hidden_size, 3))
optimizer = optimizers.SGD(lr).setup(model)
########################################################### 학습 및 결
data_size = len(x)
max_iters = data_size // batch_size

for epoch in range(max_epoch):
    idx = np.random.permutation(data_size)
    x = x[idx]
    t = t[idx]
    loss = 0

    for iters in range(max_iters):
        batch_x = x[iters * batch_size:(iters + 1) * batch_size]
        batch_t = t[iters * batch_size:(iters + 1) * batch_size]
        
        y = model(batch_x)
        loss = F.softmax_cross_entropy(y, batch_t)
        
        model.cleargrads()
        loss.backward()
        optimizer.update()
        
        loss += float(loss.data) * len(batch_t)
    
    avg_loss = loss / data_size
    print(f'Epoch {epoch+1}, loss {avg_loss:.2f}')
########################################################### 데이터셋 전처리
def f(x):
    y = x / 2.0
    return y

train_set = dezero.datasets.Spiral(transform=f)
###########################################################
Seq2Seq의 문제점과 해결책
Seq2Seq 모델에서는 입력 문장의 길이와 관계없이 고정 길이 벡터로 변환하는 문제가 있다.
이를 해결하기 위해 LSTM의 모든 은닉 상태를 활용하는 방법과 얼라이먼트를 추출하여 가중치를 구하는 방법이 있다.

########################################################### 가중치 
import numpy as np

def calculate_attention_weights(hs, ht):
    scores = np.dot(hs, ht.T)  # 내적 계산
    attention_weights = softmax(scores, axis=1)  # 소프트맥스 정규화
    return attention_weights

def softmax(x, axis=None):
    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return e_x / np.sum(e_x, axis=axis, keepdims=True)
###########################################################
내적과 소프트맥스를 이용해 가중치를 계산하고, 이를 통해 얼라이먼트를 추출하면 Seq2Seq 모델의 성능을 향상시킬 수 있다
