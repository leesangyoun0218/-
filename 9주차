ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)ì€ ê¸°ê³„ í•™ìŠµ ë° ìµœì í™” ë¬¸ì œì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.
ì´ ì•Œê³ ë¦¬ì¦˜ì€ ì£¼ì–´ì§„ í•¨ìˆ˜ì˜ ìµœì†Œê°’(ë˜ëŠ” ìµœëŒ€ê°’)ì„ ì°¾ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ”ë°, ê²½ì‚¬í•˜ê°•ë²•ì˜ ê¸°ë³¸ ê°œë…ì€ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•˜ì—¬ í•¨ìˆ˜ ê°’ì´ ê°ì†Œí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ëŠ” ê²ƒì´ë‹¤.
ì´ë ‡ê²Œí•˜ë©´ í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì— ì ì  ê°€ê¹Œì›Œì§„ë‹¤.

ê²½ì‚¬í•˜ê°•ë²•ì€ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ ì§„í–‰ëœë‹¤

1. í•¨ìˆ˜ì˜ ë§¤ê°œë³€ìˆ˜(ë˜ëŠ” ê°€ì¤‘ì¹˜)ë¥¼ ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•œë‹¤.
2. í˜„ì¬ ë§¤ê°œë³€ìˆ˜ì—ì„œì˜ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•œë‹¤.
3. ë§¤ê°œë³€ìˆ˜ë¥¼ ê¸°ìš¸ê¸°ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì´ë™ì‹œí‚¨ë‹¤. ì´ë™í•˜ëŠ” í¬ê¸°ëŠ” í•™ìŠµë¥ (learning rate)ì´ë¼ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì¡°ì ˆì´ ê°€ëŠ¥í•˜ë‹¤.
4. ê¸°ìš¸ê¸°ê°€ 0ì— ê°€ê¹Œì›Œì§ˆ ë•Œê¹Œì§€ ë˜ëŠ” ì§€ì •ëœ íšŸìˆ˜ë§Œí¼ ë°˜ë³µí•œë‹¤.

ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ğœƒ = ğœƒ âˆ’ ğœ‚ â‹… âˆ‡ğœƒ ğ½(ğœƒ)

Î¸ëŠ” ë§¤ê°œë³€ìˆ˜ë‹¤.
Î·ëŠ” í•™ìŠµë¥ ì´ë‹¤.
âˆ‡ Î¸â€‹ J(Î¸)ëŠ” ë§¤ê°œë³€ìˆ˜ Î¸ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ J(Î¸)ì˜ ê¸°ìš¸ê¸°ë‹¤.

í•™ìŠµë¥ ì˜ ì—­í• 
Î·ëŠ” ê²½ì‚¬í•˜ê°•ë²•ì˜ ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë‹¤.
í•™ìŠµë¥ ì´ ë„ˆë¬´ í¬ë©´ ìµœì†Ÿê°’ì„ ì§€ë‚˜ì³ì„œ ë°œì‚°í•  ìˆ˜ ìˆê³ , í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ìˆ˜ë ´ ì†ë„ê°€ ë§¤ìš° ëŠë ¤ì§ˆ ìˆ˜ ìˆë‹¤.
ì ì ˆí•œ í•™ìŠµë¥ ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.

ì¥ì : ë¹„êµì  ë‹¨ìˆœí•˜ê³  êµ¬í˜„ì´ ì‰¬ìš°ë©°, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì‘ë™í•œë‹¤.
ë‹¨ì : í•™ìŠµë¥  ì„ íƒì´ ì¤‘ìš”í•˜ê³ , ë¶€ì ì ˆí•œ í•™ìŠµë¥ ì€ ìˆ˜ë ´ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆë‹¤. ë˜í•œ, ë³µì¡í•œ ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ì§€ì—­ ìµœì†Ÿê°’ì— ë¹ ì§ˆ ìˆ˜ ìˆë‹¤.

ê²°ë¡ 
ê²½ì‚¬í•˜ê°•ë²•ì€ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê¸°ë³¸ì ì´ë©´ì„œë„ ê°•ë ¥í•œ ë°©ë²•ì´ë‹¤.
ë‹¤ì–‘í•œ ë³€í˜•ë“¤ì´ ì¡´ì¬í•˜ë©°, ê° ë³€í˜•ì€ íŠ¹ì • ìƒí™©ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤.
 ê¸°ê³„ í•™ìŠµ ë° ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ë° í•„ìˆ˜ì ì´ë‹¤.


ê²½ì‚¬í•˜ê°•ë²•ì„ ì´ìš©í•˜ì—¬ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•˜ëŠ” Optimizer í´ë˜ìŠ¤ë¥¼ ì„¤ê³„í•˜ê³ , ì´ë¥¼ ìƒì†ë°›ëŠ” SGD í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•œë‹¤.
Optimizer í´ë˜ìŠ¤ëŠ” ê¸°ë³¸ì ì¸ ê¸°ëŠ¥ì„ í¬í•¨í•˜ê³ , ì‹¤ì œ ë§¤ê°œë³€ìˆ˜ ê°±ì‹  ë¡œì§ì€ ìì‹ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„í•œë‹¤.

###########################################################
Optimizer í´ë˜ìŠ¤
class Optimizer:
    def __init__(self):
        self.hooks = []

    def add_hook(self, hook):
        self.hooks.append(hook)

    def update(self, params):
        for hook in self.hooks:
            hook(params)
        for param in params:
            self.update_one(param)

    def update_one(self, param):
        raise NotImplementedError()
###########################################################
SGD í´ë˜ìŠ¤
class SGD(Optimizer):
    def __init__(self, lr=0.01):
        super().__init__()
        self.lr = lr

    def update_one(self, param):
        param.data -= self.lr * param.grad.data  # ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
###########################################################
param.data -= self.lr * param.grad.data ë¬¸ë²•ì€ íŒŒì´ì¬ì—ì„œ ìê¸° ìì‹ ì„ ê°±ì‹ í•˜ëŠ” ì—°ì‚°ì´ë‹¤. ì´ëŠ” param.data = param.data - self.lr * param.grad.dataì™€ ë™ì¼í•˜í•˜ë‹¤. 
ì¦‰, ë§¤ê°œë³€ìˆ˜ param.dataì— í•™ìŠµë¥  lrê³¼ ê¸°ìš¸ê¸° param.grad.dataì˜ ê³±ì„ ë¹¼ì„œ ê°±ì‹ í•œë‹¤.

########################################################### ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì¦ˆ ìƒ
import math
import numpy as np
import matplotlib.pyplot as plt
import dezero
from dezero import optimizers
import dezero.functions as F
from dezero.models import MLP

max_epoch = 300
batch_size = 30
hidden_size = 100  # ì€ë‹‰ì¸µ ë…¸ë“œ ìˆ˜
lr = 1.0  # í•™ìŠµë¥ 

x, t = dezero.datasets.get_spiral(train=True)
model = MLP((hidden_size, 3))
optimizer = optimizers.SGD(lr).setup(model)
########################################################### í•™ìŠµ ë° ê²°
data_size = len(x)
max_iters = data_size // batch_size

for epoch in range(max_epoch):
    idx = np.random.permutation(data_size)
    x = x[idx]
    t = t[idx]
    loss = 0

    for iters in range(max_iters):
        batch_x = x[iters * batch_size:(iters + 1) * batch_size]
        batch_t = t[iters * batch_size:(iters + 1) * batch_size]
        
        y = model(batch_x)
        loss = F.softmax_cross_entropy(y, batch_t)
        
        model.cleargrads()
        loss.backward()
        optimizer.update()
        
        loss += float(loss.data) * len(batch_t)
    
    avg_loss = loss / data_size
    print(f'Epoch {epoch+1}, loss {avg_loss:.2f}')
########################################################### ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
def f(x):
    y = x / 2.0
    return y

train_set = dezero.datasets.Spiral(transform=f)
###########################################################
Seq2Seqì˜ ë¬¸ì œì ê³¼ í•´ê²°ì±…
Seq2Seq ëª¨ë¸ì—ì„œëŠ” ì…ë ¥ ë¬¸ì¥ì˜ ê¸¸ì´ì™€ ê´€ê³„ì—†ì´ ê³ ì • ê¸¸ì´ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë¬¸ì œê°€ ìˆë‹¤.
ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ LSTMì˜ ëª¨ë“  ì€ë‹‰ ìƒíƒœë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•ê³¼ ì–¼ë¼ì´ë¨¼íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.

########################################################### ê°€ì¤‘ì¹˜ 
import numpy as np

def calculate_attention_weights(hs, ht):
    scores = np.dot(hs, ht.T)  # ë‚´ì  ê³„ì‚°
    attention_weights = softmax(scores, axis=1)  # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì •ê·œí™”
    return attention_weights

def softmax(x, axis=None):
    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return e_x / np.sum(e_x, axis=axis, keepdims=True)
###########################################################
ë‚´ì ê³¼ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì´ìš©í•´ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ í†µí•´ ì–¼ë¼ì´ë¨¼íŠ¸ë¥¼ ì¶”ì¶œí•˜ë©´ Seq2Seq ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤
